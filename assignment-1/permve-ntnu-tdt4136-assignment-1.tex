\input{../templates/assignment.tex}

\title{	
\normalfont \normalsize 
\textsc{Norwegian University of Science and Technology\\TDT4136 -- Introduction to Artificial Intelligence} \\ [25pt]
\horrule{0.5pt} \\[0.4cm]
\huge Assignment 1:\\ A.I. Fundementals and Intelligence Agents\\
\horrule{2pt} \\[0.5cm]
}

\author{Per Magnus Veierland\\permve@stud.ntnu.no}

\date{\normalsize\today}

\begin{document}
\maketitle

\section*{Theoretical Questions}

\begin{enumerate}
\item \textbf{What is the Turing Test, and how is it conducted?}

The Turing Test, originally introduced as ``The Imitation Game'' by Alan~M.~Turing in his paper ``Computing Machinery and Intelligence'' from 1950, describes a experiment intended to answer the question of whether machines can think. In the experiment, a machine (\textbf{A}) and a human (\textbf{B}) communicates purely textually with a human interrogator (\textbf{C}). Through conversation with both participants, the goal of the interrogator is to discern which participant is human, and which is the machine. The objective of the human participant (\textbf{B}) is to help the interrogator conclude that it really is \textbf{B} that is human, while the machine participant is free to choose whichever strategy to convince the interrogator that it is the human.

No specific time limit was set for the experiment, but Turing estimated that a machine would by the year 2000 be able to play the game sufficiently well such that an average interrogator would not have more than a 70\% chance of correctly identifying the human participant after 5 minutes of playing. Turing did not describe a machine succeeding in the game as ``passing a test'', but described the experiment as a hypothetical and practical exercise to show that even if a machine might operate differently from a human, it can still be capable of thinking.

\item \textbf{What is the relationship between thinking rationally and acting rationally? Is rational thinking an absolute condition for acting rationally?}

Rational thinking is not an absolute condition for acting rationally. Since the rationality of actions is evaluated outside an agent, the agent might think irrationality and even act randomly, but as long as the agent's actions end up satisfying an external performance measure the agent can be said to act rationally.

\item \textbf{What is Tarski's ``theory of reference'' about?}

Alfred Tarski's ``theory of reference'' shows how to relate the objects in a logic to objects in the real world.

\item \textbf{Describe rationality. How is it defined?}

Rationality is defined as ``doing the right thing'' given the available information. Rationality describes the ability to base reasoning on logical inferences from available information and to act consistent with the available information to achieve goals established through reasoning. Within artificial intelligence a \textit{rational agent} will model the available information such that it can always perform the action available with the optimal expected outcome according to its goals. The goals of a rational agent will normally be modeled by utility functions which are used to calculate the expected outcome of different actions. Due to complexity it will often be impossible to always perform actions with optimal expected outcome, so in practice the term \textit{limited rationality} is used to describe acting appropriately when there is insufficient time to calculate the optimal answer.

\item \textbf{Consider a robot whose task it is to cross the road. Its action portfolio looks like this: look-back, look-forward, look-left-look-right, go-forward, go-back, go-left and go-right.}

\begin{enumerate}

\item \textbf{While crossing the road, a helicopter falls down on the robot and smashes it. Is the robot rational?}

Given that a) the robot's choice to cross the road was rational to begin with, and b) that the robot did not possess information indicating that a helicopter crashing into the road would be a distinct possibility, and c) that the robot was not built with sensors allowing it to sense the incoming helicopter as well as the agility necessary to avoid it; then the robot should not be faulted for the resulting outcome, and it can be said that the robot acted rationally even if the outcome was unfortunate. 

\item \textbf{While crossing the road on a green light, a passing car crashes into the robot, preventing it from crossing. Is the robot rational?}

This scenario is similar to the helicopter scenario; however this case is different both because a car driving on a road is normal and because a car running a red light has a distinct chance of occurring. A human stumbling into the road when given a green light, without further observing the environment, would easily be considered irrational since a) observing the environment to check for passing cars is within the sensing capabilities of a human, and b) observing the environment to check for passing cars has a low cost, and c) the consequence of being hit by a car is in general high and possibly fatal.

Designing a robot meant to cross a traditional road controlled by light signals without providing it with further sensing would be irresponsible. If the robot is not equipped with the sensing capabilities to sense the passing cars and its mission to cross the road is rational, then the robot cannot be faulted for being struck by a passing car. If the robot was built in a more sensible manner with the appropriate sensing capabilities it could be faulted if struck by a passing car for the same reason a human would be considered irrational. The driver of the car would still be fully at fault for running into someone while running a red light, but the robot could be considered less than fully rational for not being as cautious as it should.

\end{enumerate}

\item \textbf{Consider the vacuum cleaner world described in Chapter 2.2.1 of the textbook. Let us modify this vacuum environment so that the agent is penalized 1 point for each movement.}

In the following answers it is assumed that the robot is to be evaluated on the premise of being a rational agent, and that its performance measure is to be awarded 1 point for each clean square per time step over a lifetime of 1000 time steps, and that the robot is penalized 1 point for every movement made.

\begin{enumerate}

\item \textbf{Can a simple reflex agent be rational for this environment?} 

The rational agent theory requires the robot to make actions which maximizes its performance measure. As a simple reflex agent does not have any internal state, it will not be able to remember whether there is any dirt in its opposite location. As it can only sense the presence of dirt in its current location it must keep moving between the two locations, sucking up any detected dirt before moving on to the opposite location. 

When evaluated by the updated performance measure which places a cost on movement, the action which would maximize its performance measure would be to stop moving after cleaning both locations. This would ensure 2 points awarded for every time step for the rest of the scenario. Continuously moving between the two locations to check for dirt will cost the robot 1 point per time step without any beneficial effect. The robot's behavior can therefore not be considered rational as evaluated by the updated performance measure. 

\item \textbf{Can a reflex agent with state be rational in this environment?}

Yes, a reflex agent with state would be able to operate rationally in this environment as evaluated by the updated performance measure. Since the agent will be able to remember whether there still is dirt in the other location it can stop moving once it knows that all locations has been cleaned. See Table~\ref{table:agent_function_reflex_state} for the agent function description.

\begin{table}
\begin{center}
\begin{tabular}{cc|cc}
\multicolumn{2}{c|}{\bfseries{Input}} & \multicolumn{2}{c}{\bfseries{Output}}\\
\bfseries{Perception} & \bfseries{State} & \bfseries{Action} & \bfseries{State}\\
\hline
$\mathit{[A, Dirty]}$ & $\mathit{[Any, Any]}$ & $\mathit{Clean}$ & $\mathit{[Clean, Unchanged]}$ \\
$\mathit{[A, Clean]}$ & $\mathit{[Any, Unknown]}$ & $\mathit{Right}$ & $\mathit{[Clean, Unknown]}$ \\
$\mathit{[B, Dirty]}$ & $\mathit{[Any, Any]}$ & $\mathit{Clean}$ & $\mathit{[Unchanged, Clean]}$ \\
$\mathit{[B, Clean]}$ & $\mathit{[Unknown, Any]}$ & $\mathit{Left}$ & $\mathit{[Unknown, Clean]}$ \\
\end{tabular}
\caption{Simplified agent function (only current percepts and state is listed) for reflex agent with state. The perception input consists of the ``robot's current location'' and ``clean/dirty state of current location''. The state consists of ``clean/dirty state location A'' and ``clean/dirty state location B''. }
\label{table:agent_function_reflex_state}
\end{center}
\end{table}

\item \textbf{Assume now that the simple reflex agent (i.e., no internal state) can perceive the clean/dirty state of both locations at the same time. Can this agent be rational? Explain your answer. In case it can be rational, design the agent function.}

Yes, a simple reflex agent which can perceive the clean/dirty state of both locations at the same time, while still being able to perceive its location, will be able to act perfectly rational. With the improved sensing capabilities it will be able to perceive that there is no more work to complete and be able to rest after finishing its task, see Table~\ref{table:agent_function_reflex_percept} for the agent function.

\begin{table}
\begin{center}
\begin{tabular}{c|c}
\bfseries{Perception} & \bfseries{Action}\\
\hline
$\mathit{[A, Dirty, Any]}$ & $\mathit{Clean}$ \\
$\mathit{[A, Clean, Dirty]}$ & $\mathit{Right}$ \\
$\mathit{[B, Any, Dirty]}$ & $\mathit{Clean}$ \\
$\mathit{[B, Dirty, Clean]}$ & $\mathit{Left}$ \\
\end{tabular}
\caption{Simplified agent function (only current percepts are listed) for simple reflex agent capable of sensing all locations. The perception input consists of the ``robot's current location'', ``clean/dirty state location A'' and ``clean/dirty state location B''.}
\label{table:agent_function_reflex_percept}
\end{center}
\end{table}

\end{enumerate}

\item \textbf{Consider the vacuum cleaner environment shown in Figure 2.3 in the textbook. Describe the environment using properties from Chapter 2.3.2, e.g. episodic/sequential, deterministic/stochastic etc. Explain selected values for properties in regards to the vacuum cleaner environment.}

\begin{enumerate}

\item \textbf{Observability} -- The observability of the environment depends on the sensors of the agent. Given the original vacuum cleaner robot from chapter 2.2.1 which can only observe its current location the environment must be considered partially observable. However the improved robot from question \textit{6c} which is able to sense the state of all environment locations simultaneously would render the environment fully observable.

\item \textbf{Single-agent vs. multi-agent} -- As described, the vacuum cleaner environment will only contain one agent; the vacuum cleaner.

\item \textbf{Deterministic vs. stochastic} -- The vacuum cleaner environment is an ideal environment where each following state is completely determined by the current state and the action performed by the vacuum cleaner agent. This makes the environment deterministic.

\item \textbf{Episodic vs. sequential} -- The vacuum cleaner environment could be considered episodic as each location is considered atomically and does not depend on actions taken in other locations. For each location the agent performs its action as a direct consequence of its current percepts. There is also an element of sequentiality to the environment as the cleaning of a location can affect the decision to return to the location if the robot has the state or perception to allow this.

\item \textbf{Statis vs. dynamic} -- The vacuum cleaner environment does not change while the robot is deliberating, however as the performance score changes with time the environment can be described as semi-dynamic. If both locations are dirty and the robot spends time deliberating, the robot will lose out on attainable points compared to if it had cleaned out locations which would earn it points in the time passed.

\item \textbf{Discrete vs. continuous} -- The vacuum cleaner environment is described as an ideal environment where all properties; spatial locations, time steps, reward system, the robot's perception and the robot's actions are discrete.

\item \textbf{Known vs. unknown} -- In the vacuum cleaner environment the consequences of all possible actions are known.

Overall the vacuum cleaner environment represents an easy agent environment; \textit{fully observable}, \textit{single-agent}, \textit{deterministic}, \textit{mostly episodic}, \textit{semi-dynamic}, \textit{discrete} and \textit{known}.

\end{enumerate}

\item \textbf{Discuss the advantages and limitations of these four basic kinds of agents:}

\begin{enumerate}

\item \textbf{Simple reflex agents}

Simple reflex agents do not have state and only consider current percepts when determining their actions. They are often implemented using simple \textit{``if-then''}  rules describing the desired behavior. Simple behaviors can often be described sufficiently and transparently in this manner. However more complex behaviors which depend on sequential actions and planning can become difficult or impossible to implement without access to state and more sophisticated implementation techniques.

High degrees or full visibility is often necessary to avoid getting stuck in infinite loops which can otherwise be hard for simple reflex agents to escape. One technique to break out of loops is to introduce some randomness in decisions.

The possibility for small implementations and not requiring memory makes simple reflex agents attractive when building low-cost systems, especially in swarm configurations where more advanced group behavior can be construed from simple agent behaviors. Simple reflex agents can also be attractive in scenarios where quick responses are required and more advanced deliberation will consume too much time.

\item \textbf{Model-based reflex agents}

State combined with a model describing how the current state will be transformed by the available actions can be added to a simple reflex agent; making it a model-based reflex agent. This helps fix some of the weaknesses of a simple reflex agent since the model-based state can now keep track of parts of the environment which are not part of the agent's current percepts. In some cases it may also be possible to refactor simple reflex agent behavior rules to simpler rules when state can be used as part of the rules.

\item \textbf{Goal-based agents}

Goal-based agents are incorporated with a notion of ``desirable world states'' which the agent seeks to achieve. Having goals stated more explicitly and separate from being encoded into base behaviors is an advantage since it helps make the goals more transparent and easier to update. Goal-based agents often need some ability to plan or search for ways to apply its actions to achieve its desired world state(s), since most goals will not be achievable through a direct mapping between current percepts and actions.

\item \textbf{Utility-based agents} 

Utility provides a mechanism to quantify the desirability of outcomes. Instead of designing the behaviors of an agent to align with an external performance measure, or pursue purely binary goals; a sensible utility function can be built into an agent enabling it to perform well in more complex situations. When an agent must decide between conflicting goals and a trade-off must be found, or if the agent must decide between goals which involve uncertainties; a utility function provides a quantified mechanism to find the highest achievable expected utility from the available actions.

\end{enumerate}

\end{enumerate}

\end{document}

